{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial Intellegence HW1 \n",
    "# Jan 20, 2023\n",
    "# Booth 32200\n",
    "# Nic Marlton\n",
    "\n",
    "### Psuedo Code\n",
    "# dependencies\n",
    "# dictionary of spot ids (pulled mannually from surfline.com)\n",
    "# use spot ids list to query surfline api\n",
    "    # note: either the conditions url or the wave url could be used\n",
    "    # `conditions` returns ratings 'POOR' to 'EPIC', whereas `wave`\n",
    "    # returns an optimal rating 0, 1, 2. I chose the optimal rating\n",
    "    # given its relative simplicity\n",
    "    # helpful overview of api below:\n",
    "    # https://pkg.go.dev/github.com/mhelmetag/surflinef/v2#section-readme\n",
    "# assign surfline ratings to photos\n",
    "    # in a future itteration of this tool, I would want to pull photos\n",
    "    # and ratings in real time. that would make this step unnecessary. \n",
    "    # for the purposes of this proof of concept, I am mannually assigning \n",
    "    # ratings. \n",
    "# sort 90% of photos into 4 X 4 - good v bad X crowded v quiet\n",
    "# store remaining 10 % for test data\n",
    "\n",
    "### Process after this point occurs in Google's Teachable Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from random import randint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of spotIds \n",
    "spot_id_dict = {\n",
    "    'PlesurePoint':'5842041f4e65fad6a7708807',\n",
    "    'UpperTrestles':'5842041f4e65fad6a7708887',\n",
    "    'SanClementeStateBeach':'5842041f4e65fad6a77088cf',\n",
    "    'LowerTrestles':'5842041f4e65fad6a770888a',\n",
    "    'SteamerLane':'5842041f4e65fad6a7708805',\n",
    "    'TStreetOverview':'5842041f4e65fad6a7708830',\n",
    "    'Church':'5842041f4e65fad6a770888b',\n",
    "    'Strands':'5842041f4e65fad6a77088d5',\n",
    "    'Jacks':'5842041f4e65fad6a770880b',\n",
    "    'SaltCreekOverview':'5842041f4e65fad6a770882e',\n",
    "    # 'Hightower':'584204214e65fad6a7709cbd', // excluding Hightower given it is in FL \n",
    "    'Lowers':'5842041f4e65fad6a770888a',\n",
    "    # 'VenicePierSouthside':'5842041f4e65fad6a7708b17' // excluding Venice Pier given it is in FL\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spot_id_dict to query surfline api\n",
    "def get_wave_data(spot_id):\n",
    "    \"\"\"\n",
    "    Executes an API query to surfline.com for a given spotId, e.g.\n",
    "    'PlesurePoint':'5842041f4e65fad6a7708807'. Specifically, it uses \n",
    "    the wave url\n",
    "\n",
    "    Input:\n",
    "        - spot_id from spot_id_dict\n",
    "\n",
    "    Output:\n",
    "        - 2 column df timestamp and optimalScore for given beach.\n",
    "    \"\"\"\n",
    "    # API Call to sufrline\n",
    "    login_base_url = 'https://services.surfline.com/trusted/token'\n",
    "    base_url = 'https://services.surfline.com/kbyg/regions/forecasts/wave'\n",
    "    response = requests.request(\"GET\", base_url + '?spotId=' + spot_id + '&days=1')\n",
    "    \n",
    "    # Transform data for output in \n",
    "    df = pd.json_normalize(response.json()['data'],record_path =['wave'])\n",
    "    df = df[['timestamp','surf.optimalScore']]\n",
    "    return df.head(1)\n",
    "\n",
    "def capture_current_conditions(spot_id_dict):\n",
    "    \"\"\"\n",
    "    Captures wave data for a given day by running get_wave_data() \n",
    "    for each record in spot_id_dict. In a future version of this module\n",
    "    it could also save records to a file or db for use in photo labeling\n",
    "\n",
    "    Input: \n",
    "        - takes spot_id_dict as input.\n",
    "\n",
    "    Output:\n",
    "        - df of optimalScore and metadata for spots in spot_id_dict for a \n",
    "        given date\n",
    "    \"\"\"\n",
    "    forecast_df = pd.DataFrame()\n",
    "    for i in spot_id_dict:\n",
    "        spot_forecast_df = get_wave_data(spot_id_dict[i])\n",
    "        spot_forecast_df['spot'] = i\n",
    "        spot_forecast_df['spot_id'] = spot_id_dict[i]\n",
    "        spot_forecast_df['date'] = pd.Timestamp.today().date()\n",
    "        forecast_df = pd.concat([forecast_df,spot_forecast_df])\n",
    "\n",
    "    return forecast_df\n",
    "\n",
    "forecast_df = capture_current_conditions(spot_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample output for current date\n",
    "forecast_df.to_csv('forcast_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                filename  count\n",
      "0      Church_2021_02_19_0102pm_frame_12_left_jpg.rf....      5\n",
      "1      Church_2021_02_19_0102pm_frame_12_left_jpg.rf....     10\n",
      "2      Church_2021_02_19_0102pm_frame_12_left_jpg.rf....     12\n",
      "3      Church_2021_02_19_0102pm_frame_12_left_jpg.rf....      1\n",
      "4      Church_2021_02_19_0102pm_frame_12_left_jpg.rf....      1\n",
      "...                                                  ...    ...\n",
      "14078  UpperTrestles_2021_02_19_1253pm_frame_9_left_j...      3\n",
      "14079  UpperTrestles_2021_02_19_1253pm_frame_9_left_j...     16\n",
      "14080  UpperTrestles_2021_02_19_1253pm_frame_9_right_...      1\n",
      "14081  UpperTrestles_2021_02_19_1253pm_frame_9_right_...      1\n",
      "14082  UpperTrestles_2021_02_19_1253pm_frame_9_right_...      1\n",
      "\n",
      "[14083 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Concatenate data to create list of files w & w/o surfers \n",
    "# I did not include these csv files in the git repo because of their size.\n",
    "# they can be downlaoded with the photo data here:\n",
    "# https://universe.roboflow.com/surfline/surfer-spotting/dataset/1\n",
    "def create_file_name_df():\n",
    "    # Concatenate all file names from full Roboflow data set (~40k photos)\n",
    "    df1 = pd.read_csv('test_annotations.csv') # csv describing Roboflow test data\n",
    "    df2 = pd.read_csv('train_annotations.csv') # csv describing Roboflow train data \n",
    "    df3 = pd.read_csv('valid_annotations.csv') # csv describing Roboflow validation data\n",
    "    annotations_df = pd.concat([df1,df2,df3],ignore_index=True)\n",
    "    \n",
    "    # Remove rows for non-CA beaches and NaN\n",
    "    annotations_df['filename'] = annotations_df['filename'].apply(lambda x: np.nan \n",
    "            if str(x).__contains__('VenicePierSouthside') or str(x).__contains__('VenicePierSouthside') else x)\n",
    "    df_mask = annotations_df['filename'].isna()\n",
    "    annotations_df = annotations_df[~df_mask]\n",
    "    \n",
    "    # Get count of surfers in each file\n",
    "    annotations_df = annotations_df.groupby(['filename']).agg({'filename':'count'})#drop_duplicates().reset_index()\n",
    "    annotations_df = annotations_df.rename(columns={'filename':'count'}).reset_index()\n",
    "    print(annotations_df)\n",
    "\n",
    "# Dataframe of files that include surferss\n",
    "file_name_df = create_file_name_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: bad pattern: [surfline_api_call.py,\n"
     ]
    }
   ],
   "source": [
    "# Select 100 images with surfers and 100 images w/o surfers\n",
    "# file will code images as 0 (few surfers: < 3), or 1 (many surfers: > 2)\n",
    "# Mannually code images as 0 (poor surf), 2 (good surf) \n",
    "    # note: we are ignoring the 1 optimalScore rating available on surfline\n",
    "    # in order to simplify our classification\n",
    "def choose_200_files():\n",
    "    files_to_encode_df = pd.DataFrame()\n",
    "    for i in range (100):\n",
    "        randint(0,len())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%! ls .\n",
    "# # myvar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01_ts_code_directory-Xf6xDVFD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49e53e0ddad2980e35f979ab131222550b6caf82d4f4f17a9b066eafc1fafab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
