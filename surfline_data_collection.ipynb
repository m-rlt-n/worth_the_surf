{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial Intellegence HW1 \n",
    "# Jan 20, 2023\n",
    "# Booth 32200\n",
    "# Nic Marlton\n",
    "\n",
    "### Psuedo Code\n",
    "# dependencies\n",
    "# dictionary of spot ids (pulled mannually from surfline.com)\n",
    "# use spot ids list to query surfline api\n",
    "    # note: either the conditions url or the wave url could be used\n",
    "    # `conditions` returns ratings 'POOR' to 'EPIC', whereas `wave`\n",
    "    # returns an optimal rating 0, 1, 2. I chose the optimal rating\n",
    "    # given its relative simplicity\n",
    "    # helpful overview of api below:\n",
    "    # https://pkg.go.dev/github.com/mhelmetag/surflinef/v2#section-readme\n",
    "# assign surfline ratings to photos\n",
    "    # in a future itteration of this tool, I would want to pull photos\n",
    "    # and ratings in real time. that would make this step unnecessary. \n",
    "    # for the purposes of this proof of concept, I am mannually assigning \n",
    "    # ratings. \n",
    "# sort 90% of photos into 4 X 4 - good v bad X crowded v quiet\n",
    "# store remaining 10 % for test data\n",
    "\n",
    "### Process after this point occurs in Google's Teachable Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from random import randint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of spotIds \n",
    "spot_id_dict = {\n",
    "    'PlesurePoint':'5842041f4e65fad6a7708807',\n",
    "    'UpperTrestles':'5842041f4e65fad6a7708887',\n",
    "    'SanClementeStateBeach':'5842041f4e65fad6a77088cf',\n",
    "    'LowerTrestles':'5842041f4e65fad6a770888a',\n",
    "    'SteamerLane':'5842041f4e65fad6a7708805',\n",
    "    'TStreetOverview':'5842041f4e65fad6a7708830',\n",
    "    'Church':'5842041f4e65fad6a770888b',\n",
    "    'Strands':'5842041f4e65fad6a77088d5',\n",
    "    'Jacks':'5842041f4e65fad6a770880b',\n",
    "    'SaltCreekOverview':'5842041f4e65fad6a770882e',\n",
    "    # 'Hightower':'584204214e65fad6a7709cbd', // excluding Hightower given it is in FL \n",
    "    'Lowers':'5842041f4e65fad6a770888a',\n",
    "    # 'VenicePierSouthside':'5842041f4e65fad6a7708b17' // excluding Venice Pier given it is in FL\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spot_id_dict to query surfline api\n",
    "def get_wave_data(spot_id):\n",
    "    \"\"\"\n",
    "    Executes an API query to surfline.com for a given spotId, e.g.\n",
    "    'PlesurePoint':'5842041f4e65fad6a7708807'. Specifically, it uses \n",
    "    the wave url\n",
    "\n",
    "    Input:\n",
    "        - spot_id from spot_id_dict\n",
    "\n",
    "    Output:\n",
    "        - 2 column df timestamp and optimalScore for given beach.\n",
    "    \"\"\"\n",
    "    # API Call to sufrline\n",
    "    login_base_url = 'https://services.surfline.com/trusted/token'\n",
    "    base_url = 'https://services.surfline.com/kbyg/regions/forecasts/wave'\n",
    "    response = requests.request(\"GET\", base_url + '?spotId=' + spot_id + '&days=1')\n",
    "    \n",
    "    # Transform data for output in \n",
    "    df = pd.json_normalize(response.json()['data'],record_path =['wave'])\n",
    "    df = df[['timestamp','surf.optimalScore']]\n",
    "    return df.head(1)\n",
    "\n",
    "def capture_current_conditions(spot_id_dict):\n",
    "    \"\"\"\n",
    "    Captures wave data for a given day by running get_wave_data() \n",
    "    for each record in spot_id_dict. In a future version of this module\n",
    "    it could also save records to a file or db for use in photo labeling\n",
    "\n",
    "    Input: \n",
    "        - takes spot_id_dict as input.\n",
    "\n",
    "    Output:\n",
    "        - df of optimalScore and metadata for spots in spot_id_dict for a \n",
    "        given date\n",
    "    \"\"\"\n",
    "    forecast_df = pd.DataFrame()\n",
    "    for i in spot_id_dict:\n",
    "        spot_forecast_df = get_wave_data(spot_id_dict[i])\n",
    "        spot_forecast_df['spot'] = i\n",
    "        spot_forecast_df['spot_id'] = spot_id_dict[i]\n",
    "        spot_forecast_df['date'] = pd.Timestamp.today().date()\n",
    "        forecast_df = pd.concat([forecast_df,spot_forecast_df])\n",
    "\n",
    "    return forecast_df\n",
    "\n",
    "forecast_df = capture_current_conditions(spot_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sample output for current date\n",
    "forecast_df.to_csv('forcast_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate data to create list of files w & w/o surfers \n",
    "    # I did not include these csv files in the git repo because of their size.\n",
    "    # they can be downlaoded with the photo data here:\n",
    "    # https://universe.roboflow.com/surfline/surfer-spotting/dataset/1\n",
    "\n",
    "def create_file_name_df():\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    # Concatenate all file names from full Roboflow data set (~40k photos)\n",
    "    df1 = pd.read_csv('test_annotations.csv') # csv describing Roboflow test data\n",
    "    df2 = pd.read_csv('train_annotations.csv') # csv describing Roboflow train data \n",
    "    df3 = pd.read_csv('valid_annotations.csv') # csv describing Roboflow validation data\n",
    "    annotations_df = pd.concat([df1,df2,df3],ignore_index=True)\n",
    "    \n",
    "    # Remove rows for non-CA beaches and NaN\n",
    "    annotations_df['filename'] = annotations_df['filename'].apply(lambda x: np.nan \n",
    "            if str(x).__contains__('VenicePierSouthside') or str(x).__contains__('VenicePierSouthside') else x)\n",
    "    df_mask = annotations_df['filename'].isna()\n",
    "    annotations_df = annotations_df[~df_mask]\n",
    "    \n",
    "    # Get count of surfers in each file\n",
    "    annotations_df = annotations_df.groupby(['filename']).agg({'filename':'count'})#drop_duplicates().reset_index()\n",
    "    annotations_df = annotations_df.rename(columns={'filename':'count'}).reset_index()\n",
    "    annotations_df['crowded'] = annotations_df['count'].apply(lambda x: 1 if x > 2 else 0)\n",
    "\n",
    "    return annotations_df\n",
    "\n",
    "# Dataframe of files that include surferss\n",
    "files_w_surfers_df = create_file_name_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 3)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select 100 images with surfers and 100 images w/o surfers\n",
    "\n",
    "def choose_200_files(files_w_surfers_df):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    to_encode_df = pd.DataFrame()\n",
    "    \n",
    "    # Select 100 images with surfers\n",
    "        # We carry over count of surfers and bool indicating crowded/not\n",
    "        # from files_w_surfers_df\n",
    "        # note: 0 (few surfers: <= 2), or 1 (many surfers: > 2)\n",
    "    for i in range (100):\n",
    "        index = randint(0,len(files_w_surfers_df) - 1)\n",
    "        frame = files_w_surfers_df.iloc[index].to_frame().T\n",
    "        to_encode_df = pd.concat([to_encode_df,frame])\n",
    "    \n",
    "    # read in names of all files form .txt file\n",
    "        # again this is not saved on GitHub, but can be created based on\n",
    "        # file names in directories downloaded from Roboflow\n",
    "    all_file_names_df = pd.read_csv('all_file_names.txt',sep=\"\\s{4}\",header=None,engine='python')\n",
    "    all_file_names_df[['count','crowded']] = [0,0]\n",
    "    all_file_names_df = all_file_names_df.rename(columns={0:'filename'})\n",
    "\n",
    "    # Select 100 images without surfers\n",
    "        # We pull a random file from all_file_names_df and search for it in\n",
    "        # files_w_surfers_df. If we do not find it, we add to to_encode_df\n",
    "    count = 0\n",
    "    while count < 100:\n",
    "        index = randint(0,len(all_file_names_df) - 1)\n",
    "        file_path = all_file_names_df.iloc[index].to_frame().T\n",
    "        search_val = str(file_path.iloc[0,0])\n",
    "        if files_w_surfers_df.filename[files_w_surfers_df.filename == search_val].count() > 0:\n",
    "            count = count\n",
    "        else:\n",
    "            to_encode_df = pd.concat([to_encode_df,file_path])\n",
    "            count += 1\n",
    "\n",
    "    return to_encode_df\n",
    "\n",
    "to_encode_df = choose_200_files(files_w_surfers_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mannually code images as 0 (poor surf), 2 (good surf) \n",
    "    # note: we are ignoring the 1 optimalScore rating available on surfline\n",
    "    # in order to simplify our classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01_ts_code_directory-Xf6xDVFD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49e53e0ddad2980e35f979ab131222550b6caf82d4f4f17a9b066eafc1fafab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
